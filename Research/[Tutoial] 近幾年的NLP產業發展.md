# [Tutoial] è¿‘å¹¾å¹´çš„NLPç”¢æ¥­ç™¼å±•

- **Metadata** : `type: Tutoial` `scope: research` 
- **Techs Need** : `NLP`
- **Status**: `need-review`
<br/><br/>

## âœ¨ You should already know
- nothing

ğŸ‘©â€ğŸ’» ğŸ‘¨â€ğŸ’»

## âœ¨ About the wiki
- `Situation:` å°æ–¼å‰›å…¥é–€æˆ–æ˜¯éç›¸é—œäººå£«é›£ä»¥é€²å…¥NLPé ˜åŸŸã€‚
- `Target:` æä¾›åŸºæœ¬èªªæ˜å’Œæ–‡ä»¶æ–¹ä¾¿å°æ¥ã€‚
- `Index:`

| Sub title | decription | memo |
| ------ | ------ | ------ |
| åŸºæœ¬å®šç¾© | è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯ä¸€é–€ç ”ç©¶è¨ˆç®—æ©Ÿå¦‚ä½•ç†è§£ã€åˆ†æã€æ“ä½œå’Œç”Ÿæˆäººé¡èªè¨€çš„å­¸ç§‘ | - |
| è¿‘äº›å¹´NLPå¦‚ä½•æ‡‰ç”¨åœ¨æ•™è‚²å ´æ™¯ | è‡ªç„¶èªè¨€ç†è§£æŠ€è¡“å¯ä»¥æ‡‰ç”¨åœ¨æ•™è‚²å ´æ™¯ä¸­çš„ä¾‹å­ | - |
| ç›®å‰NLPçš„ç™¼å±•è¶¨å‹¢ | ç›®å‰æ›´å¥½åœ°æ¨¡æ“¬äººé¡çš„èªè¨€è¡Œç‚º | - |
| åƒè€ƒæ–‡ç» | - | - |


---
<br>

### **æ¦‚è¿°**
> ä»‹ç´¹æœ€è¿‘å¹¾å¹´çš„è‡ªç„¶èªè¨€è™•ç†çš„ç”¢æ¥­ç™¼ç”¢

####  ğŸ“ åŸºæœ¬å®šç¾©
> è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯ä¸€é–€ç ”ç©¶é›»è…¦æ©Ÿæ¢°æ‡‰è©²å¦‚ä½•ç†è§£ã€åˆ†æã€æ“ä½œå’Œç”Ÿæˆäººé¡èªè¨€çš„å­¸ç§‘ã€‚
> å®ƒæ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œä¹Ÿæ˜¯è¨±å¤šAIæŠ€è¡“çš„åŸºç¤ã€‚
> è¿‘å¹´ä¾†ï¼Œè‡ªç„¶èªè¨€è™•ç†æŠ€è¡“åœ¨æ‡‰ç”¨ä¸Šå–å¾—äº†é©šäººçš„é€²å±•ï¼Œå„ç¨®æ¨¡å‹æ¨é™³å‡ºæ–°ï¼Œç‰¹å®šä»»å‹™ä¸Šæº–ç¢ºåº¦ä¹Ÿä¸€ç›´éƒ½åœ¨æŒ‘æˆ°ç”šè‡³è¶…è¶Šäººé¡ã€‚
> è€Œè‡ªç„¶èªè¨€ä¸­æœ€å¸¸è¦‹çš„è™•ç†æ–¹å¼æœ‰:
> 1. æ·±åº¦å­¸ç¿’æ¨¡å‹æ¡†æ¶ï¼šæ·±åº¦å­¸ç¿’æ¨¡å‹æ¡†æ¶åŒ…æ‹¬æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDNNï¼‰ã€é•·çŸ­æœŸè¨˜æ†¶ï¼ˆLSTMï¼‰ã€å¾ªç’°ç¥ç¶“ç¶²çµ¡ï¼ˆRNNï¼‰ã€æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆAttentionï¼‰ç­‰ã€‚
> 2. è©åµŒå…¥æ¨¡å‹æ¡†æ¶ï¼šè©åµŒå…¥æ¨¡å‹æ¡†æ¶åŒ…æ‹¬word2vecã€GloVeã€FastTextç­‰ã€‚
> 3. èªè¨€æ¨¡å‹æ¡†æ¶ï¼šèªè¨€æ¨¡å‹æ¡†æ¶åŒ…æ‹¬n-gramæ¨¡å‹ã€éš±é¦¬çˆ¾å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰ã€æ¢ä»¶éš¨æ©Ÿå ´ï¼ˆCRFï¼‰ç­‰ã€‚
> 4. è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹æ¡†æ¶ï¼šè‡ªç„¶èªè¨€è™•ç†æ¨¡å‹æ¡†æ¶åŒ…æ‹¬æ¨™è¨»å™¨ï¼ˆTaggerï¼‰ã€å¥æ³•åˆ†æå™¨ï¼ˆParserï¼‰ã€èªæ„åˆ†æå™¨ï¼ˆSemantic Analyzerï¼‰ç­‰ã€‚
> 
>
> è€Œå…¶ä¸­çš„æ·±åº¦æ¨¡å‹æ¡†æ¶æ›´æ˜¯ç•¶å‰è‘—é‡ç™¼å±•ç›®æ¨™ï¼Œä¸¦é€™äº›æ¨¡å‹å¾æ¿€çƒˆçš„ç ”ç©¶é ˜åŸŸä¸­è„«ç©è€Œå‡ºï¼š
> 1. BERTï¼šBERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯Google AIç ”ç©¶å°çµ„é–‹ç™¼çš„ä¸€ç¨®è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°ç†è§£è‡ªç„¶èªè¨€ï¼Œä¸¦æä¾›æ›´å¥½çš„è‡ªç„¶èªè¨€ç†è§£èƒ½åŠ›ã€‚
>          BERTå¯ä»¥ç”¨æ–¼è©æ€§æ¨™è¨»ã€å¥å­åˆ†å‰²ã€æ–‡æœ¬åˆ†é¡å’Œå…¶ä»–è‡ªç„¶èªè¨€ä»»å‹™ã€‚[1]
>
> 2. GPTï¼šGPTï¼ˆGenerative Pre-trained Transformerï¼‰æ˜¯OpenAIé–‹ç™¼çš„ä¸€ç¨®è‡ªç„¶èªè¨€ç”Ÿæˆæ¨¡å‹ï¼ŒGPTæ˜¯ä¸€ç¨®åŸºæ–¼Transformerçš„æ¶æ§‹å’Œè¨“ç·´ç¨‹åºï¼Œç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ã€‚
>         åŸ¹è¨“éµå¾ªå…©å€‹éšæ®µçš„ç¨‹åºã€‚é¦–å…ˆï¼Œå°æœªæ¨™è¨˜çš„æ•¸æ“šä½¿ç”¨èªè¨€å»ºæ¨¡ç›®æ¨™ä¾†å­¸ç¿’ç¥ç¶“ç¶²çµ¡æ¨¡å‹çš„åˆå§‹åƒæ•¸ã€‚
>         éš¨å¾Œï¼Œé€™äº›åƒæ•¸ä½¿ç”¨ç›¸æ‡‰çš„ç›£ç£ç›®æ¨™é©æ‡‰ç›®æ¨™ä»»å‹™ã€‚[2]
>
> 3. RoBERTaï¼šRoBERTaï¼ˆRobustly Optimized BERT Pretrainingï¼‰æ˜¯Facebook AIç ”ç©¶å°çµ„é–‹ç™¼çš„ä¸€ç¨®è‡ªæˆ‘ç›£ç£æ¨¡å‹(self-supervised)ï¼Œæ ¹æ“šBERTè¨“ç·´ä¸è¶³çš„éƒ¨åˆ†é€²è¡Œæ”¹è‰¯ï¼Œä¸¦ä¸”å¯ä»¥é”åˆ°æˆ–è¶…éå…¶å¾Œç™¼å¸ƒçš„æ¯å€‹æ¨¡å‹çš„æ€§èƒ½ã€‚
>             RoBERTaåœ¨ GLUEã€RACE å’Œ SQuAD ä¸Šå–å¾—äº†æœ€å…ˆé€²çš„çµæœï¼Œé€™äº›çµæœçªå‡ºäº†ä»¥å‰è¢«å¿½è¦–çš„è¨­è¨ˆé¸æ“‡çš„é‡è¦æ€§ã€‚
>             RoBERTaä½¿ç”¨æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£è‡ªç„¶èªè¨€ï¼Œä¸¦æä¾›æ›´å¥½çš„è‡ªç„¶èªè¨€ç†è§£èƒ½åŠ›ã€‚[3]
> é€éé€™äº›æ¡†æ¶ï¼Œå¾èŠå¤©æ©Ÿå™¨äººåˆ°è‡ªç„¶èªè¨€ç†è§£ï¼Œå¾èªéŸ³è­˜åˆ¥åˆ°èªéŸ³åˆæˆï¼Œå¾æ–‡æœ¬åˆ†æåˆ°æ–‡æœ¬ç”Ÿæˆï¼Œéƒ½æœ‰äº†å¾ˆå¤§çš„çªç ´ã€‚
>
>
> è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“çš„æ‡‰ç”¨å·²ç¶“éåŠå„è¡Œå„æ¥­ï¼Œå¾é‡‘èã€æ³•å¾‹ã€é†«ç™‚ã€æ•™è‚²ã€é›»å­å•†å‹™åˆ°æœç´¢å¼•æ“ï¼Œéƒ½æœ‰è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“çš„è¹¤è·¡ã€‚
> ä¾‹å¦‚ï¼Œé‡‘èè¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†åˆ†æå®¢æˆ¶çš„æŠ•è³‡è¡Œç‚ºï¼Œä»¥æä¾›æ›´å¥½çš„æŠ•è³‡å»ºè­°ï¼›
> æ³•å¾‹è¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†å¿«é€Ÿæœç´¢æ³•å¾‹æ–‡ä»¶ï¼Œä»¥æé«˜å·¥ä½œæ•ˆç‡ï¼›
> é†«ç™‚è¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†è‡ªå‹•è­˜åˆ¥ç—…äººçš„ç—…å²ï¼Œä»¥æé«˜è¨ºæ–·çš„æº–ç¢ºæ€§ï¼›
> é›»å­å•†å‹™è¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†è‡ªå‹•å›ç­”å®¢æˆ¶çš„å•é¡Œï¼Œä»¥æé«˜å®¢æˆ¶æœå‹™æ°´å¹³ï¼›
> æœç´¢å¼•æ“è¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†æ›´å¥½åœ°ç†è§£ç”¨æˆ¶çš„æŸ¥è©¢ï¼Œä»¥æé«˜æœç´¢çµæœçš„è³ªé‡ï¼›
> æ•™è‚²è¡Œæ¥­å¯ä»¥åˆ©ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†å”åŠ©æ•™å¸«å¿«é€ŸåŒç†å­¸ç”Ÿçš„èª²ç¨‹åé¥‹ï¼Œä»¥æé«˜æ•™å­¸æ•ˆç‡ã€‚

####  ğŸ“ è¿‘äº›å¹´NLPå¦‚ä½•æ‡‰ç”¨åœ¨æ•™è‚²å ´æ™¯
> è‡ªç„¶èªè¨€ç†è§£æŠ€è¡“å¯ä»¥æ‡‰ç”¨åœ¨æ•™è‚²å ´æ™¯ä¸­çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œä¾‹å¦‚ï¼š
1. å…§å®¹è­˜åˆ¥ï¼šå¯ä»¥ç”¨æ–¼è©•ä¼°æ•™æèˆ‡èª²ç¨‹å…§å®¹ï¼Œä»¥åŠé‡å°ä¸åŒéœ€æ±‚æå‡ºè¼”åŠ©å…§å®¹ã€‚
2. è‡ªç„¶èªè¨€è™•ç†ï¼šå¯ä»¥ç”¨æ–¼è‡ªå‹•åˆ†æå­¸ç”Ÿçš„ä½œæ¥­ï¼Œæä¾›æ•™å¸«åœ¨æ‰¹æ”¹ä½œæ¥­æ™‚çš„è©•åˆ†åƒè€ƒã€‚
3. å­¸ç¿’è¼”åŠ©ï¼šå¯ä»¥ç”¨æ–¼è©•ä¼°å­¸ç”Ÿçš„å°æ–¼èª²ç¨‹çš„ç†è§£ç¨‹åº¦èˆ‡å­¸ç¿’è¡¨ç¾ï¼Œä»¥åŠæä¾›åŠæ™‚çš„åé¥‹ã€‚
4. æ™ºèƒ½åŠ©æ‰‹ï¼šå¯ä»¥ç”¨æ–¼æä¾›å­¸ç”Ÿåœ¨å­¸ç¿’éç¨‹ä¸­çš„æŒ‡å°å’Œæ”¯æŒã€‚
5. èŠå¤©æ©Ÿå™¨äººï¼šå¯ä»¥ç”¨æ–¼æä¾›å­¸ç”Ÿåœ¨å­¸ç¿’éç¨‹ä¸­çš„æŒ‡å°å’Œæ”¯æŒï¼Œä»¥åŠæä¾›ä¸€å°ä¸€çš„å»ºè­°è©•ä¼°ã€‚

####  ğŸ“ ç›®å‰NLPçš„ç™¼å±•è¶¨å‹¢
> ç›®å‰NLPçš„ç™¼å±•è¶¨å‹¢è¶Šä¾†è¶Šé è¿‘äººé¡çš„æ€ç¶­ï¼Œè¶Šä¾†è¶Šå¤šçš„æŠ€è¡“è¢«é–‹ç™¼å‡ºä¾†ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ“¬äººé¡çš„èªè¨€è¡Œç‚ºã€‚
> å…¶ä¸­ï¼Œè‡ªç„¶èªè¨€è™•ç†æŠ€è¡“çš„ç™¼å±•è¶¨å‹¢æ˜¯æ·±åº¦å­¸ç¿’ï¼Œç‰¹åˆ¥æ˜¯æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼Œå®ƒå€‘å¯ä»¥æ›´å¥½åœ°æ¨¡æ“¬äººé¡çš„èªè¨€è¡Œç‚ºï¼Œä¸¦æä¾›æ›´å¥½çš„çµæœã€‚
> æ­¤å¤–ï¼Œè©åµŒå…¥æŠ€è¡“ä¹Ÿè¢«å»£æ³›æ‡‰ç”¨æ–¼NLPï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºè©èªä¹‹é–“çš„é—œä¿‚ï¼Œä¸¦æä¾›æ›´å¥½çš„çµæœï¼Œå¸¸è¢«æ‡‰ç”¨æ–¼è©æ€§æ¨™è¨»ï¼Œå¥æ³•åˆ†æï¼Œèªç¾©åˆ†æç­‰NLPä»»å‹™ã€‚
> ç•¶å‰NLPçš„è¨“ç·´æ–¹å¼æ˜¯ä»¥Pre-train modeæ­é…transfer learningç‚ºä¸»ï¼Œå› ç‚ºPre-train modelæ“æœ‰å¤§é‡çš„è³‡æ–™è¨“ç·´å‡ºä¾†çš„æˆæœï¼Œé€étransfer learning
> å¯ä»¥è®“ä¸åŒçš„å­ä»»å‹™çš„æ¨¡å‹å¯ä»¥æ›´å¿«è¨“ç·´å‡ºä¾†ï¼Œè®“NLPæŠ€è¡“å¯ä»¥æ›´åŠ æ™®åŠï¼Œæ‡‰ç”¨åˆ°æ›´å¤šçš„é ˜åŸŸã€‚

####  ğŸ“ åƒè€ƒæ–‡ç»
1. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
2. Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.
3. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
